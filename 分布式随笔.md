**# cap

1.分区容错
2.可用性
3.一致性

# raft的一切

## 1.关键模块

领导人选举，日志复制和安全性

## 2.重点记录

1.强领导者:日志条目只能从领导者发送给其他服务器。
2.领导选举:使用一个随机计时器来选举领导者。

## 3.复制状态机

重点：
1.保证日志相同
2.5个节点的集群可以容忍两个节点的失败。

## 4.paxos的问题

说白了就太难理解了，有没有标准实现。

## 5.为了可理解性的设计

1.日志不可以有空洞
2.随机化去简化 Raft 中领导人选举算法（这一句暂时没什么直接的理解）

## 6.raft的一致性算法

1.选举领导人。由领导人负责管理日志复制的责任来实现一致性。

2.这一段主要讲了一些选举的条件

## 7.raft基础

1.5个节点最多允许失效2个（节点数为2n+1，失效容忍数不能超过节点的一半）。
2.服务的节点角色：领导人，跟随者，候选人
3.正常情况下只允许一个领导人，其他都是跟随者。
4.跟随者都是被动的，只是简单的处理领导和候选的请求。领导人处理客户端的请求
4.如果客户端的请求打到了跟随者，跟随者会把请求重定向到领导人。
5.在一个给定的任其term内只有最多只有一个领导者。
6.term任期在raft中充当逻辑时钟划分整个集群的时间线。
7.如果一个节点发现自己的任期过期了，改节点会立马变成跟随者状态。同理一个节点受到一个过期的请求，那么该节点会直接拒绝这个请求。
8.raft的rpc只有两种。投票（RequestVote）附加条目（AppendEntries）

## 8.领导人选举

1.领导者周期性的向所有跟随者发送心跳包，如果一个跟随者在一段时间里没有受到任何消息，也就是超时选举，那么他就会认为系统中没有可用的领导者，并且发起选举选出心的领导者。

2.开始选举跟随者的term需要+1，并将自己的角色转换为候选人。然后并行的向集群中的其他服务器发送投票rpc.候选人状态会保持直到三件事情之一发生
1.他自己赢得了这次选举
2.其他服务器成为了领导者
3.一段时间后没有任何人获胜（选举超时）

3.一个任期内只能投票一次，一旦某个候选人成为领导人，那么他就会给其他服务器发送心跳消息来建立自己的权威并阻止新的领导人诞生。

4.为了防止投票被均匀瓜分导致选举失败，raft使用随机选举超时时间解决。选举时间是在150-300毫秒这个区间随机选择的。

## 9.日志复制

1.日志是由指令和任期组成的，每一条指令也有自己的整数索引。

2.leader先把指令附加到日志里面去，然后同过rpc同步到其他服务器。当日志被完整的复制，领导人就会应用这条日志到状态机中，然后把执行结果返回给客户端。如果跟随者崩溃或者运行缓慢，再或者网络丢包，领导人会不断重复尝试附加日志的rpc直到所有跟随者最终都储存了所有的日志条目。

3.被应用到状态机的日志叫做已提交日志。

4.领导人的崩溃会使日志变的不一致。这种不一致的解决方案是使用强制复制领导人的日志解决的，这意味着跟随者的冲突的日志条目会被领导人的日志覆盖。

首先要开始检查第一次产生冲突的索引位置，然后删除跟随者从此往后的日志条目并复制领导人从此往后的日志，从而达到日志的一致性。

5.当一个领导人获得权利的时候首先应该初始化自己的nextIndex（自己最后的一条日志的索引加1）。

6.如果跟随者日志和领导不一致那么rpc的一致性检测就会失败，此时领导人就是减小nextIndex并进行重试，直到找到一致日志为止，之后就会像上面提到的那样删除冲突日志并重新复制领导人日志。

## 10.安全性

1.选举限制
1.如何比较两份日志的新旧？
通过最后一条日志的索引和任期比较。即任期相同的情况下日志长的日志新

## 11.集群成员变化

大概是在讲脑裂了

日志分为两种：
1.普通指令日志
2.集群配置日志

## 12.日志压缩

1.为什么要日志压缩？
因为日志会一直增长，而我们的存储资源却不是无限的。

2.快照是简单的压缩方法，raft保存最新的已提交的日志条目的索引和任期作为快照内容。

3.快照操作后的第一次附加日志请求时需要一致性检查，需要快最后提交的日志条目的任期和索引，所以这个数据需要保存在快照里。如果最新的快照保存成功就可以丢弃之前快照，论文里还说了可以丢弃整个日志。这里暂时保持怀疑。使用快照重置状态机以及加载快照的集群配置。

4.领导人给落后的跟随者发送快照安装的rpc.这样跟随就就可以追上集群的进度。这时候快照的指令一定比日志新，此时就可以丢弃整个日志了。和上面的内容对应了，解决了我的疑惑

## 13.客户端交互

1.客户端启动的时候发送的请求可能不是发给领导人的，那么当前服务器就会拒绝客户端的请求并且提供最近的领导人的信息。如果领导人崩溃了，那么客户端的请求就会超时。之后客户端重试挑选服务器的过程。

2.如果领导在提交日志之后相应客户端之前崩溃了，那么客户端会向新的领导人重试这条指令，导致指令被再次执行，解决方案请求附加唯一id。只读操作直接执行而不需要记录日志。但是一个领导人在返回结果的时候可能已经被作废了，这时候已经有新的领导人了，如果执行成功就会返回脏数据。
1.领导人在任期开始的时候需要提交一个没有操作的空日志，即只有index和term的no-op日志
2.领导人在处理制度请求的时候必须检察自己是否被废除了。

## 14.自己的感悟

1.选举就是为了保证上一任提交的完整性。
2.leader保证本任期的提交。

一个前置节点和本节点将整个流程串联起来就是一个类似链表的设计理念。

## 15.随便写点

心跳100ms，超时时间可以设置300ms-700ms吗？

# paxos

## 是什么

在分布式系统中保证多副本数据强一致的算法.

## paxos有啥用?

没有paxos的一堆机器, 叫做分布式;
有paxos协同的一堆机器, 叫分布式系统.

## 一些不靠谱的复制策略

### 主从异步复制

1. 主接到写请求
2. 主写入本磁盘
3. 主给客户端应答'ok'
4. 主复制数据到从库

缺点:主磁盘在复制前损坏,数据丢失.

### 主从同步复制

1. 主接到写请求
2. 主复制日志到从库
3. 从库可能堵塞
4. 客户端一直等待应答'ok',直到所有从库返回.

缺点:一个失联节点造成整个系统不可用.

### 主从半同步复制

1. 主接到写请求.
2. 主复制日志到从库.
3. 从库这时可能阻塞了.
4. 如果1<=x<=n个从库返回'ok',则返回客户端'ok'

缺点:可能任何从库数据都不完整.

### 多数派写

即每条数据必须写入半数以上的机器,每次读取必须检查半数以上的机器是否有这条
数据.

1. 客户端写入w>=n/2+1个节点,不需要主
2. 多数派读:w+r>n;r>=n/2+1
3. 最多容忍(n-1)/2个节点损坏

缺点:更新时数据不一致.

例子1:
node-1, node-2都写入了a=x,
下一次更新时node-2, node-3写入了a=y.
这时, 一个要进行读取a的客户端如果联系到了node-1和node-2, 它将看到2条不同的数据.
为了不产生歧义,需要添加一个全局时间戳,选择更大的时间戳,忽略更小的时间戳.
这样就看到了 a=x1,a=y2,通过比较时间戳发现y是新数据所以忽略a=x1.

例子2:
当客户端没有完成一多数派写入,客户端进程就挂掉了
a=x₁写入了node-1和node-2, a=y₂时只有node-3 写成功了,
然后客户端进程就挂掉了.
这时另一个读取的客户端来了,
如果它联系到node-1和node-2, 那它得到的结果是a=x₁.
如果它联系到node-2和node-3, 那它得到的结果是a=y₂.
整个系统对外部提供的信息仍然是不一致的.

## paxos是什么?

1. 一个可靠的存储系统:基于多数派读写
2. 每个paxos实例用来存储一个值
3. 用2轮rpc来确定一个值
4. 一个值'确定'后不能被修改
5. '确定'指被多数派接受写入
6. 强一致性

## 一些概念

Proposer 可以理解为客户端.
Acceptor 可以理解为存储节点.
Quorum 在99%的场景里都是指多数派, 也就是半数以上的Acceptor.
Round 用来标识一次paxos算法实例, 每个round是2次多数派读写:
算法描述里分别用phase-1和phase-2标识. 同时为了简单和明确,
算法中也规定了每个Proposer都必须生成全局单调递增的round,
这样round既能用来区分先后也能用来区分不同的Proposer(客户端).

1. last_rnd 是Acceptor记住的最后一次进行写前读取的Proposer(客户端)是谁, 以此来决定谁可以在后面真正把一个值写到存储中.
2. v 是最后被写入的值.
3. vrnd 跟v是一对, 它记录了在哪个Round中v被写入了.

## phase

### phase-1

首先是写前读,Proposer在Acceptor记录rnd(last_rnd),来标识那个Proposer可以写.

```yaml
request:
  rnd:int
```

```yaml
response:
  last_rnd:int
  v:"xxx"
  vrnd:int
```

当Acceptor收到phase-1的请求的时候,如果请求中的rnd比Acceptor的last_rnd小,则拒绝请求
将请求中的rnd保存在本地的last_rnd,从此这个Acceptor只接受这个phase-2请求.
返回应答,带上自己之前的last_rnd和之前已经接受的v.

proposer x收到多数个(quorum)个应答,就认为是可以继续运行的,如果没有联系到多于半数的acceptor
,整个系统就hang住了,这也是paxos生成的只能运行少于半数的节点失效.

这时Proposer面临2种情况:
所有的应答中都没有任何非空的v,这表示系统之前是干净的,没有任何值已经被其他paxos客户端
完成了写入(因为一个多数派读一定会看到一个多数派写的结果).这时Proposer X继续将它要写的值在phase-2中真正写入到多于半数的Acceptor中.

当proposer收到Acceptor返回的应答

1. 如果应答的last_rnd大于发出的rnd:退出.
2. 从所有应答中选择vrnd最大的v.
3. 如果所有应答的v都是空,可以选择自己要写入的v.
4. 如果应答不够多数派,退出.

### phase-2

Proposer X将它选定的值写入到Acceptor中, 这个值可能是它自己要写入的值, 或者是它从某个Acceptor上读到的v(修复).
即收到的最大的vrnd以及对应的v

当然这时(在X收到phase-1应答, 到发送phase-2请求的这段时间), 可能已经有其他Proposer又完成了一个rnd更大的phase-1,
所以这时X不一定能成功运行完phase-2.

Acceptor通过比较phase-2请求中的rnd, 和自己本地记录的rnd, 来确定X是否还有权写入. 如果请求中的rnd和Acceptor本地记录的rnd一样,
那么这次写入就是被允许的, Acceptor将v写入本地, 并将phase-2请求中的rnd记录到本地的vrnd中.(
所以这里的vrnd可以理解为raft的term)

1. 拒绝rnd不等于Acceptor last_rnd的请求
2. 将phase-2请求中v写入本地,记此v为'已接受的值'
3. last_rnd==rnd保证没有其他proposer在此过程中写入其他值.

### Learner

Paxos 还有一个不太重要的角色Learner, 是为了让系统完整加入的, 但并不是整个算法执行的关键角色, 只有在最后在被通知一下.

### phase-3

1. Acceptor发送phase-3到所有的Learner角色,让Learner知道一个值被确定了
2. 多数场合Proposer就是一个Learner
3. liveLock:多个Proposer并发的对一个值运行paxos的时候,可能会互相覆盖对方的rnd
   再次尝试,然后再次产生冲突,一直无法完成.

## paxos优化

### multi-paxos

paxos诞生之初为人诟病的一个方面就是每写入一个值就需要2轮rpc:phase-1和phase-2.
因此一个寻常的优化就是用一次rpc为多个paxos实例运行phase-1.

例如, Proposer X可以一次性为i₁~i₁₀这10个值, 运行phase-1,
例如为这10个paxos实例选择rnd为1001, 1002…1010. 这样就可以节省下9次rpc, 而所有的写入平均下来只需要1个rpc就可以完成了.

再加上commit概念(commit可以理解为: 值v送达到多数派这件事情是否送达到多数派了),
和组成员变更(将quorum的定义从”多于半数”扩展到”任意2个quourm必须有交集”).

### fast-paxos

fast-paxos通过增加quorum的数量来达到一次rpc就能达成一致的目的.
如果fast-paxos没能在一次rpc达成一致, 则要退化到classic paxos.

1. Proposer直接发送phase-2.
2. fast paxos的rnd是0.(0保证它一定小于任何一个classic rnd)
   .所以可以在出现冲突时安全的回退到classic paxos.
3. Acceptor只在v是空时才接受fast phase-2请求
4. 如果发生冲突,回退到classic-paxos,开始用一个rnd>0来运行.

fast-paxos为了能在退化成classic paxos时不会选择不同的值,
就必须扩大quorum的值. 也就是说fast-round时, quorum的大小跟classic paxos的大小不一样. 同样我们先来看看为什么

在上述classic paxos的过程描述中,我们了解了,当发生冲突的时候会选择
vrnd最大的值写入来做一次冲突解决.但是fast-paxos中所有的rnd都是0,
根本无法区分.

所以fast-paxos的quorum为n*3/4.降低了可用性,至少需要5个Acceptor,
才能容忍一个Acceptor不可用.

### phase-2Acceptor可以接受rnd>=last_rnd的请求

# 多数派读写的少数派实现

## 前言

paxos可以看作是2次 多数派读写完成一次强一致性读写,多数派要求半数以上的参与者接受某笔操作,
但多数派读写并不一定需要多余半数以上参与者.分布式系统中某些场合的优化,可以通过减少参与者数量来完成

## 多数派读写:分布式系统的基础

分布式系统中, 其中一个基础的问题是如何在不可靠硬件(低可用性)基础上构建可靠(高可用性)的服务, 要达成这个目标,
核心的手段就是复制(例如一份数据存3个副本).
而复制过程中的一致性问题,最后都归结为paxos的解决方案.

## 解决的问题

多数派读写解决了,半同步复制数据不一致的问题.每条数据必须写入到半数以上的机器上. 每次读取数据都必须检查半数以上的机器上是否有这条数据.
在这种策略下, 数据可靠性足够, 宕机容忍足够, 任一机器故障也能读到全部数据.