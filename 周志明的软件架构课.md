__# 单体系统

优点:不会发生进程间的通信,部署简单
缺点:缺少隔离和自治能力,当发生一些内存错误就会导致
过度消耗进程空间内的公共资源.需要停机维护.

# 事件驱动架构

在子系统之间建立一套事件队列管道（Event Queues），
来自系统外部的消息将以事件的形式发送到管道中，
各个子系统可以从管道里获取自己感兴趣、可以处理的事件消息，
也可以为事件新增或者是修改其中的附加信息，
甚至还可以自己发布一些新的事件到管道队列中去。

# 无服务Serverless

只涉及了后端设施（Backend）和函数（Function）两块内容.
后端设施:数据库,消息队列等
函数:一些功能化的函数,目前ai场景居多,比如对图片的各种操作.

# rpc

rpc主要的三个问题

1. 如何表示数据:独立于语言的数据格式(json,Protocol Buffers).
2. 如何传递数据(网络tcp,udp).
3. 如何表示方法(方法名)

# RESTful

一种面向资源的表达方式.是一种表达风格.
REST 的基本思想是面向资源来抽象问题，它与此前流行的面向过程的编程思想，在抽象主体上有本质的差别。

# 本地事务

数据库状态一致性.
原子性:同一个事中,对多个数据的修改,要么同时成功,要么一起被撤销.
隔离性:不同事务处理过程中,事务保证了各自业务正在读写的数据相互独立,不会彼此影响
持久性:事务应当保证所有被成功提交的数据修改都能正确地被持久化,不丢失数据.
A,I,D是手段.C是目的(一致性).

## 原子性,持久性

适用于单个数据,单个数据源.
为了能够崩溃恢复就需要存储日志:redo log,bin log
首先，日志一旦成功写入 Commit Record，那整个事务就是成功的，即使修改数据时崩溃了，重启后根据已经写入磁盘的日志信息恢复现场、继续修改数据即可，这保证了持久性。
其次，如果日志没有写入成功就发生崩溃，系统重启后会看到一部分没有 Commit Record
的日志，那将这部分日志标记为回滚状态即可，整个事务就像完全没有发生过一样，这保证了原子性。

数据的修改必须在提交日志之后,为了提高性能,在事务(日志)提交前
就修改数据,需要添加undo log,这样在数据修到一半,需要恢复的时候
就可找到之前的数据.

FORCE：当事务提交后，要求变动数据必须同时完成写入则称为 FORCE，如果不强制变动数据必须同时完成写入则称为 NO-FORCE。
现实中绝大多数数据库采用的都是 NO-FORCE 策略，只要有了日志，变动数据随时可以持久化，从优化磁盘 I/O 性能考虑，没有必要强制数据写入立即进行。
STEAL：在事务提交前，允许变动数据提前写入则称为 STEAL，不允许则称为 NO-STEAL。
从优化磁盘 I/O 性能考虑，允许数据提前写入，有利于利用空闲 I/O 资源，也有利于节省数据库缓存区的内存。

## 隔离性

现代数据库一般都会提供三种锁

1. 写锁:也叫x锁,排他锁,持有写锁的事务,才能对数据写入操作,数据加持写锁的时候,其他事务不能修改,也不施加读锁
2. 读锁:也叫s锁,多个事务可以对同一个数据添加读锁,数据加上读锁之后就无法再添加写锁.如果只有一个事务添加了读锁,可以升级为写锁
3. 范围锁:gap lock+next key

### 可重复读

加的读锁写锁,在事务结束后释放

### 读已提交

写锁会在事务结束后释放,读锁在操作结束后释放

### 读未提交

只加写锁,不加读锁

### mvcc

mvcc是一种读取优化策略,"无锁"是指不需要加读锁,思路是任何修改都不会覆盖之前的数据,
而是生成一个新副本与老版本共存.

MVCC 是只针对“读 + 写”场景的优化，如果是两个事务同时修改数据，
即“写 + 写”的情况，那就没有多少优化的空间了，加锁几乎是唯一可行的解决方案。

# 全局事务(xa,2pc)

适用于单个服务,多个数据源的事务解决方案.

为了解决多数据源事务分开提交,导致无法回滚的问题.xa将事务提交拆分成了两阶段过程,
也就是准备阶段(prepared)和提交阶段(commit).

## 准备阶段(prepared)

对数据库来说记录redo log,但是不写入最后一条commit record.这意为着在做完数据的持久化之后
并不会立即释放隔离性,也就是仍然持有锁,维持数据对其他非事务内观察者的隔离状态.

## 提交阶段(commit)

如果在准备阶段收到了所有事务参与者回复的prepared就会首先在本地持久化事务状态为commit,
然后向所有参与者发送commit指令,所有参与者立即执行提交操作.否则,任意一个参与者回复了
non-prepared消息,或任意一个参与者超时未回复,协调者都会将自己的事务状态修改为abort,
参与者立即执行回滚操作.

对于数据库来说，提交阶段的提交操作是相对轻量的，仅仅是持久化一条 Commit Record 而已，通常能够快速完成。
回滚阶段则相对耗时，收到 Abort 指令时，需要根据回滚日志清理已提交的数据，这可能是相对重负载操作。

## 保证一致性的两个前提=

1. 网络在提交阶段是可靠的,即提交阶段不会丢失消息.准备阶段失败的了可以回滚,提交阶段失败了无法不久
2. 由于网络分区,机器崩溃导致失联的节点最终一定会恢复.不会永久失联

![img.png](data%2Fimg.png)

## 2pc的缺点

1. 单点问题:参与者等待协调者指令时无法做超时处理,一旦协调者宕机,所有参与者就会一直等待
2. 性能问题:两阶段提交过程,相当于所有参与者被绑定成一个统一调度,
   期间经历两次远程调用,三次持久化(写redo log,持久化协调者状态,提交阶段写入
   commit record).整个过程将持续到参与者集群中最慢的那一个处理结束为止.
3. 一致性风险:当网络稳定性和宕机恢复能力的假设不成立时,两阶段提交会出现一致性问题

# 3pc

3pc解决了2pc的单点和性能问题,但没有解决一致性问题.

3pc把2pc的准备阶段再细分为两个阶段.

1. CanCommit:询问阶段,协调者让每个参与者的数据库评估该事务是否有可能顺利完成.
2. PreCommit
3. 把commit修改为DoCommit

在事务需要回滚的场景中性能会比二阶段好.提交的性能都很差,因为多了询问阶段
所以3pc性能更差.

因为有了CanCommit阶段,所以事务失败回滚的几率变小了,如果在平PreCommit阶段
发生了宕机,参与者没有收到DoCommit的话,默认策略是提交事务而不是持续等待,或者
失败回滚.这样就避免了协调者单点的问题.由于默认提交,所以加剧了数据不一致的问题

# 共享事务

多个服务,一个数据源.

多服务,多进程,通过一个额外的进程共享数据库链接

![img_1.png](data%2Fimg_1.png)

# 分布式事务

## 总览图
| 分布式事务模式 | AT模式                    | TCC模式                      | Saga模式             | XA模式             |
|---------|-------------------------|----------------------------|--------------------|------------------|
| 原理      | 基于数据库代理，利用undo/redo log | 业务代码实现Try/Confirm/Cancel方法 | 一系列局部事务+补偿操作组成全局事务 | 两阶段提交协议配合数据库XA协议 |
| 开发成本    | 低（对业务代码无侵入）             | 高（需为每个操作实现三个接口）            | 中（需定义补偿服务）         | 中（需要数据库支持XA协议）   |
| 数据一致性   | 强一致性                    | 最终一致性（正确实现补偿逻辑时）           | 最终一致性              | 强一致性             |
| 性能      | 受undo log大小和并发量影响       | 较好，但依赖业务逻辑设计               | 较好，可异步执行           | 差，存在阻塞问题         |
| 适用场景    | 简单事务操作，数据库支持            | 复杂业务场景，精准资源控制              | 长事务链路，异步处理         | 多个支持XA协议的数据库资源   |
| 回滚机制    | undo log自动回滚            | 业务代码手动编写Cancel逻辑           | 执行补偿服务             | 依赖数据库的XA回滚       |
| 单点故障    | 事务协调器故障影响事务             | 不直接影响业务流程                  | 不直接影响业务流程          | 协调器故障影响全局事务      |


多服务,多数据源

## 可靠事件队列(最大努力交付)

本地事务=业务操作+消息表
定时轮询消息表处理跨服务事务,成功修改消息状态,失败不断定时重试

缺点:不断重试,没有隔离性科研

## tcc

一种业务侵入性较强的方案,要求业务处理过程中必须拆分为

1. 预留业务资源
2. 确认/释放消费资源

这两个子过程

### tcc实现

1. try:尝试执行阶段，完成所有业务可执行性的检查（保障一致性），并且预留好事务需要用到的所有业务资源（保障隔离性）。
2. confirm:确认执行阶段,不进行任何业务检查,直接使用try阶段准备的资源来完成业务处理
   ,confirm阶段可能会重复执行,需要保证幂等.
3. cancel:取消执行阶段,释放try阶段预留的业务资源.cancel也可能重复执行
   也需要保证幂等

![img_2.png](data%2Fimg_2.png)

### tcc缺点

1. 业务的侵入性太强，目前一般使用Seata框架来减少编码工作量。
2. 必须要冻结资源

## saga

SAGA 事务基于数据补偿代替回滚的解决思路

1. 分布式事务t，分解成本地子事务t1，t2，ti。
2. 每个子事务设计补偿动作c1,c2,ci。
3. ti和ci必须幂等。
4. ci必须提交成功，不断重试或者人工介入。

t1-ti全部提交完成就算事务完成。否则就要执行两种恢复策略

### 正向恢复

t1-ti不断重试

### 逆向恢复

一直执行 Ci 对 Ti 进行补偿，直至成功为止（最大努力交付）。
这里要求 Ci 必须（在持续重试后）执行成功。反向恢复的执行模式为：T1，T2，…，Ti（失败），Ci（补偿），…，C2，C1。

### 补充

saga系统本身也会崩溃，所以他需要类似日志的机制，saga log来做奔溃后恢复。

## at

AT 事务是参照了 XA 两段提交协议来实现的

在业务数据提交的时候，自动拦截所有sql，分别保存sql对数据修改前后结果的快照，生成行锁，通过本地事务一起提交到
数据源中，相当于自动做了重做和回滚日志。

如果需要回滚就根据日志自动生成补偿的逆向sql。

### 代价

大幅度牺牲了隔离性甚至影响到了原子性。为了避免脏写添加了一个全局锁来实现写隔离。
要求本地事务提交之前，一定要先拿到针对修改记录的全局锁后才允许提交，而在没有获得全局锁之前就必须一直等待

# 域名解析系统

dns:域名地址 转化为 ip
dns采用udp传输,在ipv4下,未分片的udp数据包最大有效字节是512,最多可以存放13组地址记录.

# 浏览器客户端缓存

使用header:cache-control控制

# 传输链路

tcp是面对长时间,大传输来设计的.
http则是数量多,时间短,资源小.切换快.

## keep-alive

让客户端对同一个域名长期持有一个或者多个会用用完即断的tcp连接.

### 副作用

1. 队首阻塞:因为是单个tcp链接,返回多个数据需要一定顺序,如果
   第一个请求就阻塞后面的请求都会等待

## http 2.0

在http 2.0中 frame 是最小单位,每个frame都有一个
stream id用来表述属于那个stream,在一个tcp链接中
传输多个frame可以通过stream id区分出来,重组出不同
的http请求和响应报文来.

## udp

基于udp的http 3.0(quic)

# 内容分发网络（CDN，Content Distribution Network 或 Content Delivery Network)

## 路由解析

用户访问站点的时候,先发起一个为命中缓存的dns查询,域名服务商解析出
cname后,返回给本地dns.本地dns查询的时候根据cname,查询到cdn服务商
架设的权威dns,权威dns根据一些策略,返回一个合适的ip替换成源站ip,返回
给本地dns.

## 内容分发

1. 主动分发(push):源站发起,主动推送到cdn上.
2. 被动回源(pull):用户触发,由源站到cdn,cdn到用户.

## 其他作用

安全防御、资源修改、功能注入，等等。

# 负载均衡的工作模式

从形式上来说都可以分为两种：四层负载均衡和七层负载均衡。
四层负载均衡的优势是性能高，七层负载均衡的优势是功能强。

![img_3.png](data%2Fimg_3.png)

## 数据链路层负载均衡

修改数据帧中的mac地址,让数据到达实际的物理服务器,实际物理服务器的虚拟ip地址
配置为和负载均衡器一样的ip.这样服务器就可以给用户直接返回请求的响应.

数据链路层负载均衡的工作模式是，只有请求会经过负载均衡器，而服务的响应不需要从负载均衡器原路返回，整个请求、转发、响应的链路形成了一个“三角关系”.
二层负载均衡器直接改写目标 MAC 地址的工作原理，决定了它与真实服务器的通讯必须是二层可达的。通俗地说，就是它们必须位于同一个子网当中，无法跨
VLAN。

## 网络层负载均衡

在第三层网络层传输的单位是分组数据包（Packets).以ipv4为例子
一个 IP 数据包由 Headers 和 Payload 两部分组成，Headers 长度最大为 60 Bytes，它是由 20 Bytes 的固定数据和最长不超过 40
Bytes 的可选数据组成的。我们只需要知道在headers中带有源ip和目标ip即可,根据
二层负载的思路,我们可以通过修改ip地址来达成数据的转发.

1. 第一种是保持原来的数据包不变，新创建一个数据包，把原来数据包的 Headers 和 Payload 整体作为另一个新的数据包的
   Payload，在这个新数据包的 Headers 中，写入真实服务器的 IP 作为目标地址，然后把它发送出去。
   当然，尽管因为要封装新的数据包，IP 隧道的转发模式比起直接路由的模式，效率会有所下降，但因为它并没有修改原有数据包中的任何信息，所以
   IP 隧道的转发模式仍然具备三角传输的特性，即负载均衡器转发来的请求，可以由真实服务器去直接应答，无需再经过均衡器原路返回。
2. 改变目标数据包的方式：直接把数据包 Headers 中的目标地址改掉，修改后原本由用户发给均衡器的数据包，也会被三层交换机转发送到真实服务器的网卡上，而且因为没有经过
   IP 隧道的额外包装，也就无需再拆包了。但是源ip发生了改变,客户端无法识别,所以需要再次通过负载均衡器把源ip改回去,才能被客户端识别
   即(NAT模式).

## 应用层负载均衡

四层负载往上是代理模式,即客户端->负载均衡器->目标服务器,三者之间由两条独立的tcp
通道来维持通讯.
四层本身还是可以做转发操作的.

# 服务端缓存的三种属性

## 在软件开发中，引入缓存的负面作用要明显大于硬件的缓存

1. 从开发角度来说，引入缓存会提高系统的复杂度，因为你要考虑缓存的失效、更新、一致性等问题（硬件缓存也有这些问题，只是不需要由你来考虑，主流的
   ISA 也都没有提供任何直接操作缓存的指令）；
2. 从运维角度来说，缓存会掩盖掉一些缺陷，让问题在更久的时间以后，出现在距离发生现场更远的位置上；
3. 安全角度来说，缓存可能泄漏某些保密数据，这也是容易受到攻击的薄弱点。

## 引入缓存的原因

1. 缓解cpu压力
2. 缓解i/o压力

## 如何避免竞争、提高吞吐量

1. Guava Cache 为代表的同步处理机制。即在访问数据时一并完成缓存淘汰、
   统计、失效等状态变更操作，通过分段加锁等优化手段来尽量减少数据竞争。
2. Caffeine 为代表的异步日志提交机制。这种机制参考了经典的数据库设计理论，
   它把对数据的读、写过程看作是日志（即对数据的操作指令）的提交过程。
   在 Caffeine 的实现中，还设有专门的环形缓存区（Ring Buffer，也常称作 Circular
   Buffer），来记录由于数据读取而产生的状态变动日志。而且为了进一步减少数据竞争，Caffeine 给每条线程（对线程取
   Hash，哈希值相同的使用同一个缓冲区）都设置了一个专用的环形缓冲。然后，从 Caffeine 读取数据时，数据本身会在其内部的
   ConcurrentHashMap 中直接返回，而数据的状态信息变更，就存入了环形缓冲中，由后台线程异步处理。
   而如果异步处理的速度跟不上状态变更的速度，导致缓冲区满了，那此后接收的状态的变更信息就会直接被丢弃掉，直到缓冲区重新有了富余。
   所以，通过环形缓冲和容忍有损失的状态变更，Caffeine 大幅降低了由于数据读取而导致的垃圾收集和锁竞争，因而 Caffeine
   的读取性能几乎能与 ConcurrentHashMap 的读取性能相同。 另外你要知道，在向 Caffeine
   写入数据时，还要求要使用传统的有界队列（ArrayQueue）来存放状态变更信息，写入带来的状态变更是无损的，不允许丢失任何状态。
   这是考虑到许多状态的默认值必须通过写入操作来完成初始化，因此写入会有一定的性能损失。根据
   Caffeine 官方给出的数据，相比 ConcurrentHashMap，Caffeine 在写入时大约会慢 10% 左右。

## 命中率与淘汰策略

1. FIFO（First In First Out）
   即优先淘汰最早进入被缓存的数据。FIFO
   的实现十分简单，但一般来说，它并不是优秀的淘汰策略，因为越是频繁被用到的数据，往往越会早早地被存入缓存之中。所以如果采用这种淘汰策略，很可能会大幅降低缓存的命中率。
2. LRU（Least Recent Used）
   LinkedHashMap来实现。也就是，它以 HashMap 来提供访问接口，保证常量时间复杂度的读取性能；以 LinkedList
   的链表元素顺序来表示数据的时间顺序，在每次缓存命中时，
   把返回对象调整到 LinkedList 开头，每次缓存淘汰时从链表末端开始清理数据。
   LRU的缺点就是热点数据一段时间不被访问还是会被淘汰.
3. LFU（Least Frequently Used）
   即优先淘汰最不经常使用的数据。LFU 会给每个数据添加一个访问计数器，每访问一次就加 1，当需要淘汰数据的时候，就清理计数器数值最小的那批数据。
   缺点:计数器的+1会降低吞吐量,第二不便于处理随时间变化的热度变化,比如某个曾经频繁访问的数据现在不需要了，它也很难自动被清理出缓存。
4. TinyLFU（Tiny Least Frequently Used）
   TinyLFU 首先采用 Sketch 结构，来分析访问数据。所谓的
   Sketch，它实际上是统计学中的概念，即指用少量的样本数据来估计全体数据的特征。这种做法显然牺牲了一定程度的准确性，但是只要样本数据与全体数据具有相同的概率分布，Sketch
   得出的结论仍不失为一种在高效与准确之间做好权衡的有效结论。
   另外，为了解决 LFU 不便于处理随时间变化的热度变化问题，TinyLFU 采用了基于“滑动时间窗”（在第 38
   讲中我们会更详细地分析这种算法）的热度衰减算法。简单理解就是每隔一段时间，便会把计数器的数值减半，以此解决“旧热点”数据难以清除的问题。
5. W-TinyLFU（Windows-TinyLFU）
   W-TinyLFU 又是 TinyLFU 的改进版本。TinyLFU 在实现减少计数器维护频率的同时，也带来了无法很好地应对稀疏突发访问的问题。
   所谓的稀疏突发访问，是指有一些绝对频率较小，但突发访问频率很高的数据，比如某些运维性质的任务，也许一天、一周只会在特定时间运行一次，其余时间都不会用到，那么此时
   TinyLFU 就很难让这类元素通过 Sketch 的过滤，因为它们无法在运行期间积累到足够高的频率。
   W-TinyLFU 的具体做法是，把新记录暂时放入一个名为 Window Cache 的前端 LRU 缓存里面，让这些对象可以在 Window Cache
   中累积热度，如果能通过 TinyLFU 的过滤器，再进入名为 Main Cache 的主缓存中存.
   Caffeine 官方还制定了另外两种高级淘汰策略，ARC（Adaptive Replacement Cache）和LIRS（Low Inter-Reference Recency Set）。

# 分布式缓存如何与本地缓存配合

## 缓存击穿(单个热点数据)

1. 加同步锁,意味着只有第一个请求可以从数据源把数据加载到缓存.
2. 手动管理更新,失效,避免策略的自动管理.

## 缓存雪崩(大批数据段时间一起失效)

1. 提升缓存系统可用性，建设分布式缓存的集群。
2. 启用透明多级缓存，各个服务节点的一级缓存中的数据，通常会具有不一样的加载时间，这样做也就分散了它们的过期时间。
3. 将缓存的生存期从固定时间改为一个时间段内的随机时间，比如原本是一个小时过期，那可以在缓存不同数据时，设置生存期为 55 分钟到
   65 分钟之间的某个随机时间。

## 缓存污染

1. 据时，先读缓存，缓存没有的话，再读数据源，然后将数据放入缓存，再响应请求。
2. 写数据时，先写数据源，然后失效（而不是更新）掉缓存。

# 分布式共识(上)

## paxos

分布式随笔已经写过笔记了,这里就不写了.

# 分布式共识(下)

## 分布式系统中如何对某个值达成一致

可以把它分为下面三个子问题来考虑：

1. 如何选主（Leader Election）
2. 如何把数据复制到各个节点上（Entity Replication）
3. 如何保证过程是安全的（Safety）

## Gossip

以n次方的形式,在集群中广播,达到最终一致性.

1. 全量同步让节点信息一致
2. 增量同步,只对外发送变更信息,减少网络开销

### 缺点

1. 非强一致,无法估算广播时间
2. 消息冗余,不可避免的重复发送给同一个节点.